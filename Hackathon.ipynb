{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import unidecode\n",
    "import langdetect\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF \n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "data = pd.read_csv('Data/labeled_data.csv', engine='python', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the features we are interested in by dropping the useless columns\n",
    "data.drop(['battery_overheat','camera','connectivity','memory_storage','sound','water_damage'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data/labeled_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selecting and fine tuning a method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there is only English comments in the dataset.\n",
    "\n",
    "def detectEnglish(text):\n",
    "    try :\n",
    "        return(langdetect.detect(text) == 'en')\n",
    "    except :\n",
    "        return('issue')   \n",
    "    \n",
    "to_keep = data.text.apply(detectEnglish)\n",
    "\n",
    "# Issues spotted --> remove them\n",
    "to_drop = data.iloc[to_keep[to_keep == 'issue'].index]\n",
    "data = data.drop(to_drop.index)\n",
    "\n",
    "data.to_csv('Data/labeled_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data between train & test for fine tunign the algorithms before making the predictions.\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=42, stratify=data.issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stopwords dictionnary :\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Keep the negative adverbs\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to keep the negative indicators (e.g. wouldn't --> keep not). \n",
    "# So we need to expand common English contractions\n",
    "# To do so, we use a bit of code from StackOverFlow\n",
    "\n",
    "\n",
    "\n",
    "# this code is not mine! i shamelessly copied it from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# all credits go to alko and arturomp @ stack overflow.\n",
    "# basically, it's a big find/replace.\n",
    "\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = str(string)\n",
    "    # lower_case\n",
    "    string = string.lower()\n",
    "    # remove accents\n",
    "    string = unidecode.unidecode(string)\n",
    "    # expand English contractions\n",
    "    string = expandContractions(string)\n",
    "    # remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    string = pattern.sub('', string)\n",
    "    \n",
    "    \n",
    "    # remove iphone\n",
    "    pattern = re.compile(r'iphone')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove apple\n",
    "    pattern = re.compile(r'apple')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove samsung\n",
    "    pattern = re.compile(r'samsung')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove galaxy\n",
    "    pattern = re.compile(r'galaxy')\n",
    "    string = pattern.sub('', string)\n",
    "    \n",
    "    \n",
    "    # remove \\n\n",
    "    string = string.replace('\\n', ' ')\n",
    "    # remove special caracters like \"ï£¿\" and punctuation\n",
    "    string = re.sub('[^A-Za-z0-9 ]','', string)\n",
    "    # lematize\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"v\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"a\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string)\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TFIDF on 1,2,3 - grams\n",
    "\n",
    "TFIDF = TfidfVectorizer(\n",
    "      input='content',\n",
    "      lowercase=False,\n",
    "      preprocessor=preprocessing,\n",
    "      ngram_range=(1,3))\n",
    "\n",
    "# Compute the TFIDF matrix (+create a dictionnary ...)\n",
    "tfidf_train = TFIDF.fit_transform(df_train.text)\n",
    "tfidf_test = TFIDF.transform(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension by using NMF\n",
    "n_dimensions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF dimensionality reduction \n",
    "NMF_model = NMF(n_components=n_dimensions, random_state=42, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "X_train = pd.DataFrame(NMF_model.fit_transform(tfidf_train))\n",
    "X_test = pd.DataFrame(NMF_model.transform(tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Building a predictive model for each issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach consists in building 7 binary-classifiers (one for each issue). As the data is very noisy with a lot of comments that do not talk about any issue, we find a lot of False Positives (comments actuallly related to an issue, but categorized as not talking about the issue).\n",
    "\n",
    "The business case we are treating consists in filtering the enormous amount of data produced everyday on social media for identifying smartphone issues that might weaken the position of a smartphone manufacturer. \n",
    "\n",
    "Therefore our target metric is the F-score for predictions of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = pd.DataFrame(index = data.columns[1:8], columns = ['learning_rate','max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apps_update</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battery_life_charging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerservice</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locking_system</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_bugs</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      learning_rate max_depth\n",
       "apps_update                     NaN       NaN\n",
       "battery_life_charging           NaN       NaN\n",
       "customerservice                 NaN       NaN\n",
       "locking_system                  NaN       NaN\n",
       "screen                          NaN       NaN\n",
       "software_bugs                   NaN       NaN\n",
       "system                          NaN       NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apps_update being treated\n",
      "battery_life_charging being treated\n",
      "customerservice being treated\n",
      "locking_system being treated\n",
      "screen being treated\n",
      "software_bugs being treated\n",
      "system being treated\n"
     ]
    }
   ],
   "source": [
    "# After having tried a couple algorithms (random forest, SVM, AdaBoost, GradientBoosting) we decided to use\n",
    "# GradientBoosting\n",
    "\n",
    "# The following chunk helps us fine-tuning the hyperparameters --> it takes a while\n",
    "\n",
    "learning_rate_list = np.linspace(0.5,2,16)\n",
    "depth_list = [3,4]\n",
    "\n",
    "to_tune = []\n",
    "\n",
    "for element in itertools.product(learning_rate_list,depth_list):\n",
    "    to_tune += [element]\n",
    "    \n",
    "\n",
    "\n",
    "for issue in hyperparameters.index :\n",
    "    \n",
    "    print(issue, \"being treated\")\n",
    "    \n",
    "    f_score = []\n",
    "    \n",
    "    y_train_i = df_train[issue]\n",
    "    y_test_i = df_test[issue]\n",
    "    \n",
    "    for param in to_tune :\n",
    "        model = GradientBoostingClassifier(n_estimators=1000, random_state=42,\\\n",
    "                                           learning_rate=param[0],\\\n",
    "                                           max_depth=param[1])\n",
    "        model.fit(X_train, y_train_i)\n",
    "        f_score += [float(classification_report(y_test_i, model.predict(X_test)).split()[12])]\n",
    "    \n",
    "    index_best_param = np.argmax(f_score)\n",
    "    best_param = to_tune[index_best_param]\n",
    "    \n",
    "    hyperparameters.loc[issue] = best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apps_update</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battery_life_charging</th>\n",
       "      <td>0.7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerservice</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locking_system</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_bugs</th>\n",
       "      <td>1.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>1.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      learning_rate max_depth\n",
       "apps_update                       2         3\n",
       "battery_life_charging           0.7         4\n",
       "customerservice                 0.9         3\n",
       "locking_system                  0.9         3\n",
       "screen                            1         3\n",
       "software_bugs                   1.4         4\n",
       "system                          1.5         4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters.to_csv(\"data/hyperparameters.csv\")\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making predictions with our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn = pd.read_csv('Data/labeled_data.csv', engine='python', encoding = 'utf-8')\n",
    "df_predict = pd.read_csv('data/test_data.csv', engine='python', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the variables that we do not have to predict in df_predict\n",
    "to_remove = [issue for issue in df_predict.columns\\\n",
    "           if issue not in hyperparameters.index]\n",
    "# But we definitely keep 'text' \n",
    "to_remove.remove('text')\n",
    "\n",
    "df_predict.drop(to_remove, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TfidfVectorizer(\n",
    "      input='content',\n",
    "      lowercase=False,\n",
    "      preprocessor=preprocessing,\n",
    "      ngram_range=(1,3))\n",
    "\n",
    "tfidf_learn = TFIDF.fit_transform(df_learn.text)\n",
    "tfidf_predict = TFIDF.transform(df_predict.text)\n",
    "\n",
    "n_dimensions = 40\n",
    "NMF_model = NMF(n_components=n_dimensions, random_state=42, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "X_learn = pd.DataFrame(NMF_model.fit_transform(tfidf_learn))\n",
    "X_predict = pd.DataFrame(NMF_model.transform(tfidf_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apps_update being predicted\n",
      "battery_life_charging being predicted\n",
      "customerservice being predicted\n",
      "locking_system being predicted\n",
      "screen being predicted\n",
      "software_bugs being predicted\n",
      "system being predicted\n"
     ]
    }
   ],
   "source": [
    "# Make the predictions\n",
    "for issue in hyperparameters.index :\n",
    "    \n",
    "    print(issue, \"being predicted\")\n",
    "    \n",
    "    y_learn_i = df_learn[issue]\n",
    "    param = list(hyperparameters.loc[issue])\n",
    "    model = GradientBoostingClassifier(n_estimators=1000, random_state=42,\\\n",
    "                                            learning_rate=param[0],\\\n",
    "                                            max_depth=param[1])\n",
    "    model.fit(X_learn, y_learn_i)\n",
    "    \n",
    "    df_predict[issue] = model.predict(X_predict)\n",
    "\n",
    "# Output the results\n",
    "df_predict.to_csv('Data/predicted_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* apps_update *************************\n",
      "Nb of issues predicted: 119.0\n",
      "\n",
      "\n",
      "it processes quickly and takes great pictures, but the battery life is not as good as my s7 active's was at 2 years old.\n",
      "good battery life.\n",
      "the battery lasts much longer than the s6 edge, the index finger security option is very convenient for on the go, and i love the orchid gray color!\n",
      "i could not even just pay the difference for an s8+ with (theoretically) 30% more battery life.\n",
      "impressed with updated features, including improved camera and increased battery life.\n",
      "great product definetly recommend to anyone great battery life\n",
      "been using this phone for a few months now, generally happy with the performance and battery life.\n",
      "great features, beautiful design and execution and for me significantly improved battery life over my previous galaxy s5!\n",
      "i have noticed the battery holds its charge much longer, but i suspect that would be true for any new phone compared to one that's almost two years old.\n",
      "the battery is great and the provided charger charges the phone fast.\n",
      "\n",
      "\n",
      "************************* battery_life_charging *************************\n",
      "Nb of issues predicted: 124.0\n",
      "\n",
      "\n",
      "i still like my phone, the size, the features, but the battery leaves something to be desired.\n",
      "the battery life is at least ten hours.\n",
      "the battery lasts so much longer too!\n",
      "got it for my fiancÃ©e's birthday and she loves it, feels great in the hand and has amazing battery life\n",
      "beautiful screen, good battery life and android...\n",
      "it does everything well and battery life is impressive.\n",
      "the battery life of this phone is phenomenal.\n",
      "battery runs hot.\n",
      "at the end of a day busy with calls, texts, and reviewing email, it still retains over have it's battery capacity.\n",
      "battery is incredible\n",
      "\n",
      "\n",
      "************************* customerservice *************************\n",
      "Nb of issues predicted: 11.0\n",
      "\n",
      "\n",
      "i needed to update my galaxy s6 and overall pleased with the s8.\n",
      "initially i purchased the verizon version.\n",
      "i have always loved my galaxy's, i had two of them before this one.\n",
      "and each one is something helpfully to my daily use?\n",
      "verizon put through a software update that rendered the facial recognition unusable and effectively locked me out of my screen.\n",
      "the pros out weight the cos on this one.\n",
      "my new samsung s-8 has performed excellently.\n",
      "i'd had that phone for almost two years (i chose to upgrade early by three months due to a technical issue with the old phone).\n",
      "got the s8 a couple months ago and i have no problems\n",
      "however, the one area that they (samsung) have preached is bixby.\n",
      "\n",
      "\n",
      "************************* locking_system *************************\n",
      "Nb of issues predicted: 15.0\n",
      "\n",
      "\n",
      "i don't use the fingerprint reader due to new location, and have hit the bixby button several times accidentally, but battery life is great as well as picture quality.\n",
      "the size is great to me but yes, it took some time to get used to.\n",
      "hopefully more apps will integrate face id from touch id as i have had to enter passwords where i previously used touch id as that is not an option as of yet.\n",
      "i usually hate upgrading my phone because it takes me a while to get used to a new one.\n",
      "and face id has been more accurate than touch id for me (not that touch id was a problem.\n",
      "- face id - its fine but not as easy to use as touch id.\n",
      "there are some new features that take some time to get used to.\n",
      "faceid has only failed me if my eyes werenât open or if the phone isnât able to see my face because of where i am when it tries to scan me).\n",
      "the best vr phone i've used.\n",
      "aside from the oled screen, selfie camera and face id the phone isn't much different.\n",
      "\n",
      "\n",
      "************************* screen *************************\n",
      "Nb of issues predicted: 13.0\n",
      "\n",
      "\n",
      "even if you donr like or prefer having a phone case, get one.even the back of this phone is made of glass (not gorilla glass).\n",
      "wish the edges were more compatible with a glass protector but i've adapted to using just a skinomi and it looks great.\n",
      "a very very small scratch or crack under the screen coming from the notch area at the top of the phone coming down maybe 1/2 a cm long.\n",
      "the camera takes amazing pictures !\n",
      "i love the extra screen and the actual size of the phone not to big or to small.\n",
      "- physical size - this is what sold me on the iphone x instead of the iphone 8 plus.\n",
      "the price of this phone is ridiculous and the car charger from my previous samsung s5 does not work with this phone without the clumsy and frustrating adapter that is included with it.\n",
      "the iphone 6+ was too big.\n",
      "the phone size is about the best you get in a modern smartphone if you are one who does not wish to carry around a tablet as a phone.\n",
      "the extra body youâre used to with the top and bottom bezels make you feel like youâre downsizing but you have to remember that the x screen is bigger (technically) so youâre mainly just loosing the extra none usable space (other than the home button since that was absolutely usable).\n",
      "\n",
      "\n",
      "************************* software_bugs *************************\n",
      "Nb of issues predicted: 46.0\n",
      "\n",
      "\n",
      "the battery life of this phone is phenomenal.\n",
      "the s8 has a great camera and many features to make it easy to use.\n",
      "am learning a new benefit daily.\n",
      "no issues with verizon service, as opposed to the previous carrier i had.\n",
      "but so far i have no complaints about the s8.\n",
      "there is no reason for exclusivity like that!\n",
      "used to have an iphone and wanted to try something new.\n",
      "my favorite thing is the new anamojis!\n",
      "it is just a crazy step up from the previous phone i had (iphone 5s).\n",
      "few complaints though: the phone does get laggy and glitchy sometimes, like once a week.\n",
      "\n",
      "\n",
      "************************* system *************************\n",
      "Nb of issues predicted: 26.0\n",
      "\n",
      "\n",
      "i was extremely disappointed and i called samsung customer service directly, they fixed the screen at no cost to me.\n",
      "not happy with this situation.\n",
      "for one, i was almost late getting up for work because my alarm didnât go off.\n",
      "it is easy to hold, easy to use, must i say beautiful.great deal at best buy!\n",
      "it isn't horrible but i think it could be better.\n",
      "i needed a new sim card the same day i bought the phone, and i blame the tray scratching it.\n",
      "took a few days to get used to the changes between phones, but haven't had any problems beyond that.\n",
      "as of today, itâs the only phone that checks all of my needs: excellent rear camera, excellent front camera, water proof, dust proof, wireless (fast) charging, full-day battery life, a headphone jack, a micro sd card slot, fingerprint scanner, and after using the s8, bluetooth 5.0.\n",
      "the curved screen on this phone has no chance against any kind of wear and tear.\n",
      "fits nice in my hand, light and easy to use.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for issue in hyperparameters.index :\n",
    "    print(\"*************************\", issue, \"*************************\")\n",
    "    s = sum(df_predict[issue])\n",
    "    print(\"Nb of issues predicted:\",s)\n",
    "    print('\\n')\n",
    "    to_print = random.sample(list(df_predict[df_predict[issue]==1].text), 10)\n",
    "    for elt in to_print:\n",
    "        print(elt)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we see that we do not managed to predict as well as expected with our simple model, indeed :\n",
    "* A lot of positive comments fall between the cracks of the models\n",
    "* Some negative comments are ill-classified (e.g. this comment was categorized as `software_bugs` but would have better been categorised as `locking_system` : _\"the cons include unlocking my phone with finger prints doesn't always work\"_.)\n",
    "\n",
    "However, issues regarding `screen` and `locking_system` seem to be quite well categorized.\n",
    "\n",
    "\n",
    "We believe that the points mentionned are due to the simplicity of the model, the lack of enough training data and the structure of the data :\n",
    "* an overwhelming ratio of positive comments in what was scrapped. We think that people should have limited themselves to scrap negative comments by using metadata (number of stars, number of angry reacts on FB, etc.).\n",
    "* `screen` and `locking_system` are features that people often complaint so the models captured them more easily.\n",
    "* As the topic `system` is very vague, we think that a lot of people put in it issues that did'nt match a specific category. This is why we predict fairly well negative comments in it ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
