{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import unidecode\n",
    "import langdetect\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF \n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "data = pd.read_csv('Data/labeled_data.csv', engine='python', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 297 lines not relevant anymore\n"
     ]
    }
   ],
   "source": [
    "# Keep only the features we are interested in by dropping the useless columns\n",
    "data.drop(['battery_overheat','camera','connectivity','memory_storage','sound','water_damage'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data/labeled_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selecting and fine tuning a method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there is only English comments in the dataset.\n",
    "\n",
    "def detectEnglish(text):\n",
    "    try :\n",
    "        return(langdetect.detect(text) == 'en')\n",
    "    except :\n",
    "        return('issue')   \n",
    "    \n",
    "to_keep = data.text.apply(detectEnglish)\n",
    "\n",
    "# Issues spotted --> remove them\n",
    "to_drop = data.iloc[to_keep[to_keep == 'issue'].index]\n",
    "data = data.drop(to_drop.index)\n",
    "\n",
    "data.to_csv('Data/labeled_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data between train & test for fine tunign the algorithms before making the predictions.\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=42, stratify=data.issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stopwords dictionnary :\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Keep the negative adverbs\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to keep the negative indicators (e.g. wouldn't --> keep not). \n",
    "# So we need to expand common English contractions\n",
    "# To do so, we use a bit of code from StackOverFlow\n",
    "\n",
    "\n",
    "\n",
    "# this code is not mine! i shamelessly copied it from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# all credits go to alko and arturomp @ stack overflow.\n",
    "# basically, it's a big find/replace.\n",
    "\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = str(string)\n",
    "    # lower_case\n",
    "    string = string.lower()\n",
    "    # remove accents\n",
    "    string = unidecode.unidecode(string)\n",
    "    # expand English contractions\n",
    "    string = expandContractions(string)\n",
    "    # remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    string = pattern.sub('', string)\n",
    "    \n",
    "    \n",
    "    # remove iphone\n",
    "    pattern = re.compile(r'iphone')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove apple\n",
    "    pattern = re.compile(r'apple')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove samsung\n",
    "    pattern = re.compile(r'samsung')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove galaxy\n",
    "    pattern = re.compile(r'galaxy')\n",
    "    string = pattern.sub('', string)\n",
    "    \n",
    "    \n",
    "    # remove \\n\n",
    "    string = string.replace('\\n', ' ')\n",
    "    # remove special caracters like \"ï£¿\" and punctuation\n",
    "    string = re.sub('[^A-Za-z0-9 ]','', string)\n",
    "    # lematize\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"v\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"a\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string)\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TFIDF on 1,2,3 - grams\n",
    "\n",
    "TFIDF = TfidfVectorizer(\n",
    "      input='content',\n",
    "      lowercase=False,\n",
    "      preprocessor=preprocessing,\n",
    "      ngram_range=(1,3))\n",
    "\n",
    "# Compute the TFIDF matrix (+create a dictionnary ...)\n",
    "tfidf_train = TFIDF.fit_transform(df_train.text)\n",
    "tfidf_test = TFIDF.transform(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension by using NMF\n",
    "n_dimensions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF dimensionality reduction \n",
    "nmf_model = NMF(n_components=n_dimensions, random_state=42, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "X_train = pd.DataFrame(nmf_model.fit_transform(tfidf_train))\n",
    "X_test = pd.DataFrame(nmf_model.transform(tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Building a predictive model for each issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach consists in building 7 binary-classifiers (one for each issue). As the data is very noisy with a lot of comments that do not talk about any issue, we find a lot of False Positives (comments actuallly related to an issue, but categorized as not talking about the issue).\n",
    "\n",
    "The business case we are treating consists in filtering the enormous amount of data produced everyday on social media for identifying smartphone issues that might weaken the position of a smartphone manufacturer. \n",
    "\n",
    "Therefore our target metric is the F-score for predictions of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = pd.DataFrame(index = data.columns[1:7], columns = ['learning_rate','max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apps_update</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battery_life_charging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerservice</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locking_system</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_bugs</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      learning_rate max_depth\n",
       "apps_update                     NaN       NaN\n",
       "battery_life_charging           NaN       NaN\n",
       "customerservice                 NaN       NaN\n",
       "locking_system                  NaN       NaN\n",
       "screen                          NaN       NaN\n",
       "software_bugs                   NaN       NaN"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apps_update being treated\n",
      "battery_life_charging being treated\n",
      "customerservice being treated\n",
      "locking_system being treated\n",
      "screen being treated\n",
      "software_bugs being treated\n"
     ]
    }
   ],
   "source": [
    "# After having tried a couple algorithms (random forest, SVM, AdaBoost, GradientBoosting) we decided to use\n",
    "# GradientBoosting\n",
    "\n",
    "# The following chunk helps us fine-tuning the hyperparameters --> it takes a while\n",
    "\n",
    "learning_rate_list = np.linspace(0.5,2,16)\n",
    "depth_list = [3,4]\n",
    "\n",
    "to_tune = []\n",
    "\n",
    "for element in itertools.product(learning_rate_list,depth_list):\n",
    "    to_tune += [element]\n",
    "    \n",
    "\n",
    "\n",
    "for issue in hyperparameters.index :\n",
    "    \n",
    "    print(issue, \"being treated\")\n",
    "    \n",
    "    f_score = []\n",
    "    \n",
    "    y_train_i = df_train[issue]\n",
    "    y_test_i = df_test[issue]\n",
    "    \n",
    "    for param in to_tune :\n",
    "        model = GradientBoostingClassifier(n_estimators=1000, random_state=42,\\\n",
    "                                           learning_rate=param[0],\\\n",
    "                                           max_depth=param[1])\n",
    "        model.fit(X_train, y_train_i)\n",
    "        f_score += [float(classification_report(y_test_i, model.predict(X_test)).split()[12])]\n",
    "    \n",
    "    index_best_param = np.argmax(f_score)\n",
    "    best_param = to_tune[index_best_param]\n",
    "    \n",
    "    hyperparameters.loc[issue] = best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apps_update</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battery_life_charging</th>\n",
       "      <td>0.7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerservice</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locking_system</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_bugs</th>\n",
       "      <td>1.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      learning_rate max_depth\n",
       "apps_update                       2         3\n",
       "battery_life_charging           0.7         4\n",
       "customerservice                 0.9         3\n",
       "locking_system                  0.9         3\n",
       "screen                            1         3\n",
       "software_bugs                   1.4         4"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters.to_csv(\"data/hyperparameters.csv\")\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Building a predictive model for 'issue' in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making predictions with our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn = pd.read_csv('Data/labeled_data.csv', engine='python', encoding = 'utf-8')\n",
    "df_predict = pd.read_csv('data/test_data.csv', engine='python', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the variables that we do not have to predict in df_predict\n",
    "to_remove = [issue for issue in df_predict.columns\\\n",
    "           if issue not in hyperparameters.index]\n",
    "# But we definitely keep 'text' \n",
    "to_remove.remove('text')\n",
    "\n",
    "df_predict.drop(to_remove, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TfidfVectorizer(\n",
    "      input='content',\n",
    "      lowercase=False,\n",
    "      preprocessor=preprocessing,\n",
    "      ngram_range=(1,3))\n",
    "\n",
    "tfidf_learn = TFIDF.fit_transform(df_learn.text)\n",
    "tfidf_predict = TFIDF.transform(df_predict.text)\n",
    "\n",
    "n_dimensions = 40\n",
    "NMF_model = NMF(n_components=n_dimensions, random_state=42, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "X_learn = pd.DataFrame(nmf_model.fit_transform(tfidf_learn))\n",
    "X_predict = pd.DataFrame(nmf_model.transform(tfidf_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apps_update being predicted\n",
      "battery_life_charging being predicted\n",
      "customerservice being predicted\n",
      "locking_system being predicted\n",
      "screen being predicted\n",
      "software_bugs being predicted\n"
     ]
    }
   ],
   "source": [
    "# Make the predictions\n",
    "for issue in hyperparameters.index :\n",
    "    \n",
    "    print(issue, \"being predicted\")\n",
    "    \n",
    "    y_learn_i = df_learn[issue]\n",
    "    param = list(hyperparameters.loc[issue])\n",
    "    model = GradientBoostingClassifier(n_estimators=1000, random_state=42,\\\n",
    "                                            learning_rate=param[0],\\\n",
    "                                            max_depth=param[1])\n",
    "    model.fit(X_learn, y_learn_i)\n",
    "    \n",
    "    df_predict[issue] = model.predict(X_predict)\n",
    "\n",
    "# Output the results\n",
    "df_predict.to_csv('Data/predicted_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* apps_update *************************\n",
      "Nb of issues predicted: 119.0\n",
      "\n",
      "\n",
      "great phone used to lg but this is so much better love it\n",
      "got it for my fiancÃ©e's birthday and she loves it, feels great in the hand and has amazing battery life\n",
      "this phone has an amazing screen, great features and decent battery life!\n",
      "battery life is atrocious!!!\n",
      "great camera, good battery life and wireless charging were great features but the kicker was the price!\n",
      "i'm not to sure if the edge is good or bad, but good so far.\n",
      "great phone, i bought it for 500 $ \n",
      "\n",
      "\n",
      "fast, good battery, nice quality camera\n",
      "the battery is great and the provided charger charges the phone fast.\n",
      "purchased samsung 8 for husband was an upgrade and heâs completely happy with it he says has cool new features and is better screen\n",
      "it processes quickly and takes great pictures, but the battery life is not as good as my s7 active's was at 2 years old.\n",
      "\n",
      "\n",
      "************************* battery_life_charging *************************\n",
      "Nb of issues predicted: 124.0\n",
      "\n",
      "\n",
      "battery lasts a long time.\n",
      "great battery life.\n",
      "the battery lasts much longer than the s6 edge, the index finger security option is very convenient for on the go, and i love the orchid gray color!\n",
      "the battery life seems to improve with time.\n",
      "been using this phone for a few months now, generally happy with the performance and battery life.\n",
      "battery life is very good which was important to me.\n",
      "battery life is pretty good, plenty of good apps, the curved screen looks amazing.\n",
      "fast, lots of storage (64gb) and long battery life\n",
      "had an iphone for a long time.\n",
      "would like to replace battery myself.\n",
      "\n",
      "\n",
      "************************* customerservice *************************\n",
      "Nb of issues predicted: 11.0\n",
      "\n",
      "\n",
      "the pros out weight the cos on this one.\n",
      "initially i purchased the verizon version.\n",
      "however, the one area that they (samsung) have preached is bixby.\n",
      "and each one is something helpfully to my daily use?\n",
      "my new samsung s-8 has performed excellently.\n",
      "i needed to update my galaxy s6 and overall pleased with the s8.\n",
      "got the s8 a couple months ago and i have no problems\n",
      "i'd had that phone for almost two years (i chose to upgrade early by three months due to a technical issue with the old phone).\n",
      "i have always loved my galaxy's, i had two of them before this one.\n",
      "iâm an a ios fan, but must admit that my sons phone itâs pretty awesome, pic's are far way better than even iphone 8+.\n",
      "\n",
      "\n",
      "************************* locking_system *************************\n",
      "Nb of issues predicted: 15.0\n",
      "\n",
      "\n",
      "the best vr phone i've used.\n",
      "- face id - its fine but not as easy to use as touch id.\n",
      "only down side is that the bixby button takes getting used to.\n",
      "the size is great to me but yes, it took some time to get used to.\n",
      "there are some new features that take some time to get used to.\n",
      "i usually hate upgrading my phone because it takes me a while to get used to a new one.\n",
      "used to have an iphone and wanted to try something new.\n",
      "the upgrade process was pretty painless thanks to them both being samsungs, although my battery was pretty close to dead when i got it home after they transferred it for me at the local bb.\n",
      "i never thought id try another smart phone besides the iphone, but i must say samsung has done quite well with this phone i am happy to say that i am now an android user.\n",
      "faceid has only failed me if my eyes werenât open or if the phone isnât able to see my face because of where i am when it tries to scan me).\n",
      "\n",
      "\n",
      "************************* screen *************************\n",
      "Nb of issues predicted: 13.0\n",
      "\n",
      "\n",
      "it is smaller than the 7 plus but you donât lose screen size.\n",
      "the camera and picture quality is very clear.\n",
      "even if you donr like or prefer having a phone case, get one.even the back of this phone is made of glass (not gorilla glass).\n",
      "a very very small scratch or crack under the screen coming from the notch area at the top of the phone coming down maybe 1/2 a cm long.\n",
      "the iphone 6+ was too big.\n",
      "wish the edges were more compatible with a glass protector but i've adapted to using just a skinomi and it looks great.\n",
      "the extra body youâre used to with the top and bottom bezels make you feel like youâre downsizing but you have to remember that the x screen is bigger (technically) so youâre mainly just loosing the extra none usable space (other than the home button since that was absolutely usable).\n",
      "the phone size is about the best you get in a modern smartphone if you are one who does not wish to carry around a tablet as a phone.\n",
      "the price of this phone is ridiculous and the car charger from my previous samsung s5 does not work with this phone without the clumsy and frustrating adapter that is included with it.\n",
      "i love the extra screen and the actual size of the phone not to big or to small.\n",
      "\n",
      "\n",
      "************************* software_bugs *************************\n",
      "Nb of issues predicted: 46.0\n",
      "\n",
      "\n",
      "unfortunately no one can fix it no one knows why.\n",
      "the s8 has a great camera and many features to make it easy to use.\n",
      "we needed a new phone carrier and with that new phones.\n",
      "the cons include unlocking my phone with finger prints doesn't always work.\n",
      "no one else did.\n",
      "don't by new chargers for older models if you are planning on purchasing a new phone.\n",
      "no issues had for over a month so far- great camera.\n",
      "he has no complaints\n",
      "iâm an a ios fan, but must admit that my sons phone itâs pretty awesome, pic's are far way better than even iphone 8+.\n",
      "still in awe of my new x after 2 weeks of use.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for issue in hyperparameters.index :\n",
    "    print(\"*************************\", issue, \"*************************\")\n",
    "    s = sum(df_predict[issue])\n",
    "    print(\"Nb of issues predicted:\",s)\n",
    "    print('\\n')\n",
    "    to_print = random.sample(list(df_predict[df_predict[issue]==1].text), 10)\n",
    "    for elt in to_print:\n",
    "        print(elt)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we see that we do not managed to predict as well as expected with our simple model, indeed :\n",
    "* A lot of positive comments fall between the cracks of the models\n",
    "* Some negative comments are ill-classified (e.g. this comment was categorized as `software_bugs` but would have better been categorised as `locking_system` : _\"the cons include unlocking my phone with finger prints doesn't always work\"_.)\n",
    "\n",
    "However, issues regarding `screen` and `locking_system` seem to be quite well categorized.\n",
    "\n",
    "\n",
    "We believe that the points mentionned are due to the simplicity of the model, the lack of \"Big\" data and the structure of the data :\n",
    "* an overwhelming ratio of positive comments in what was scrapped. We think that people should have limited themselves to scrap negative comments by using metadata (number of stars, number of angry reacts on FB, etc.).\n",
    "* `screen` and `locking_system` are features that people often complaint so the models captured them more easily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
