{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling the dataset\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF \n",
    "\n",
    "#Natural language processing + kmeans clustering + dimensionality reduction functionalities\n",
    "import nltk\n",
    "import re\n",
    "import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim # for solid LDA implementation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and deal with issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon</td>\n",
       "      <td>Love my S8! Awesome screen and takes great pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reddit</td>\n",
       "      <td>I mean, while I don't think that's an especial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bestbuy</td>\n",
       "      <td>It would appear that this \"open box\" like new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>During all ur review u just talked about getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youtube</td>\n",
       "      <td>I really want to trade my 7 plus for the S8 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twitter</td>\n",
       "      <td>And not sold on iPhone X camera performance in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twitter</td>\n",
       "      <td>܀�܉_܀�܉�܀_܀�܀�܁����܁�܁�܉���܉�܀_܀�܀�܉�܀_܀�܉�_�_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bestbuy</td>\n",
       "      <td>Though iPhones are beginning to feel like the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bestbuy</td>\n",
       "      <td>I love my Samsung S8 edge. It is sleek and eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bestbuy</td>\n",
       "      <td>Fast, smooth as silk internet browsing. Best i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                                               text\n",
       "0   amazon  Love my S8! Awesome screen and takes great pic...\n",
       "1   reddit  I mean, while I don't think that's an especial...\n",
       "2  bestbuy  It would appear that this \"open box\" like new ...\n",
       "3  youtube  During all ur review u just talked about getti...\n",
       "4  youtube      I really want to trade my 7 plus for the S8 ?\n",
       "5  twitter  And not sold on iPhone X camera performance in...\n",
       "6  twitter  ܀�܉_܀�܉�܀_܀�܀�܁����܁�܁�܉���܉�܀_܀�܀�܉�܀_܀�܉�_�_...\n",
       "7  bestbuy  Though iPhones are beginning to feel like the ...\n",
       "8  bestbuy  I love my Samsung S8 edge. It is sleek and eas...\n",
       "9  bestbuy  Fast, smooth as silk internet browsing. Best i..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset \n",
    "scrapping_data = pd.read_csv('data_scraping_V2.csv', sep=',',engine=\"python\")\n",
    "scrapping_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Define a stopwords dictionnary :\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# We keep the negative adverbs\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not you get it?\n",
      "i am not got time for you alls foolishness\n",
      "you will not live to see tomorrow.\n",
      "you have got serious cojones coming in here like that.\n",
      "i had not've enough\n"
     ]
    }
   ],
   "source": [
    "# We want to keep the negative indicators (e.g. wouldn't --> keep not). \n",
    "# So we need to expand common English contractions\n",
    "# To do so, we use a bit of code from StackOverFlow\n",
    "\n",
    "\n",
    "\n",
    "# this code is not mine! i shamelessly copied it from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# all credits go to alko and arturomp @ stack overflow.\n",
    "# basically, it's a big find/replace.\n",
    "\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())\n",
    "\n",
    "# examples\n",
    "print (expandContractions('Don\\'t you get it?'))\n",
    "print (expandContractions('I ain\\'t got time for y\\'alls foolishness'))\n",
    "print (expandContractions('You won\\'t live to see tomorrow.'))\n",
    "print (expandContractions('You\\'ve got serious cojones coming in here like that.'))\n",
    "print (expandContractions('I hadn\\'t\\'ve enough'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = str(string)\n",
    "    # lower_case\n",
    "    string = string.lower()\n",
    "    # remove accents\n",
    "    string = unidecode.unidecode(string)\n",
    "    # expand English contractions\n",
    "    string = expandContractions(string)\n",
    "    # remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove iphone\n",
    "    pattern = re.compile(r'iphone')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove apple\n",
    "    pattern = re.compile(r'apple')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove samsung\n",
    "    pattern = re.compile(r'samsung')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove galaxy\n",
    "    pattern = re.compile(r'galaxy')\n",
    "    string = pattern.sub('', string)\n",
    "    # remove \\n\n",
    "    string = string.replace('\\n', ' ')\n",
    "    # remove special caracters like \"\" and punctuation\n",
    "    string = re.sub('[^A-Za-z0-9 ]','', string)\n",
    "    # lematize\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"v\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string,\"a\")\n",
    "    string = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(string)\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* A regular example *************\n",
      "Before: Love my S8! Awesome screen and takes great pictures!; Love my S8! Awesome screen and takes great pictures!\n",
      "----------------------------------------------\n",
      "After: love s8 awesome screen takes great pictures love s8 awesome screen takes great pictures\n",
      "\n",
      "\n",
      "************* An encoding error example *************\n",
      "Before: ܀�܉_܀�܉�܀_܀�܀�܁����܁�܁�܉���܉�܀_܀�܀�܉�܀_܀�܉�_�_܉�܉�܉���_��¬�_�__�܁����܉�܁�܁�܉�GALAXY�__�_�܉�_�_܉�܉�܉�܁���\n",
      "܉�܀_܀�܀�܉�܀_܀�܁��_�___�_�_��_�����܁_܀�܉_܀_܀�΍�����ˇ܁�\n",
      "----------------------------------------------\n",
      "After:  v\n"
     ]
    }
   ],
   "source": [
    "### Some sanity check\n",
    "\n",
    "print(\"************* A regular example *************\")\n",
    "print(\"Before: \"+scrapping_data.iloc[0].text)\n",
    "print('----------------------------------------------')\n",
    "print(\"After: \"+preprocessing(scrapping_data.iloc[0].text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"************* An encoding error example *************\")\n",
    "print(\"Before: \"+scrapping_data.iloc[6].text)\n",
    "print('----------------------------------------------')\n",
    "print(\"After: \"+preprocessing(scrapping_data.iloc[6].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_mono = TfidfVectorizer(\n",
    "      input='content',\n",
    "      lowercase=False,\n",
    "      preprocessor=preprocessing)\n",
    "\n",
    "# Compute the TFIDF matrix (+create a dictionnary ...)\n",
    "tfidf = TFIDF_mono.fit_transform(scrapping_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<46806x40690 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 603545 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('TDFIDF_matrix.npy', tfidf)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Have acces to the vocabulary mapping : words >>> numbers\n",
    "\n",
    "### Uncomment the following line\n",
    "# TFIDF_mono.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dailyreal\n",
      "['dailyreal' 'dair' 'dak']\n"
     ]
    }
   ],
   "source": [
    "# Have acces to the vocabulary mapping : numbers >>> words\n",
    "\n",
    "def getWord(search_column) :\n",
    "    for word, value in TFIDF_mono.vocabulary_.items():\n",
    "        if value == search_column:\n",
    "            search_word = word\n",
    "            break\n",
    "    return(search_word)\n",
    "print(getWord(10000))\n",
    "\n",
    "getWords = np.vectorize(getWord)\n",
    "print(getWords([10000,10001,10002]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionnality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png\"/ width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=20, init='random') \n",
    "W = model.fit_transform(tfidf) \n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46806, 2)\n",
      "(46806, 40690)\n",
      "(46806, 20)\n",
      "(20, 40690)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(scrapping_data))\n",
    "print(np.shape(tfidf))\n",
    "print(np.shape(W))\n",
    "print(np.shape(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nArgMax(array,n):\n",
    "    arraybis = array.copy()\n",
    "    arg = []\n",
    "    for i in range(n):\n",
    "        argmaxi = np.argmax(arraybis)\n",
    "        arg += [argmaxi]\n",
    "        arraybis[argmaxi] = -np.Inf\n",
    "    return(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** topic n°0********************\n",
      "['phone' 'loves' 'everything' 'got' 'bought' 'happy' 'old' 'fast' 'far'\n",
      " 'perfect']\n",
      "\n",
      "\n",
      "******************** topic n°1********************\n",
      "['face' 'get' 'button' 'id' 'home' 'used' 'works' 'getting' 'recognition'\n",
      " 'touch']\n",
      "\n",
      "\n",
      "******************** topic n°2********************\n",
      "['s8' 's7' 'note' 'watching' 'android' 'oreo' 'edge' 's6' 'plus' 'got']\n",
      "\n",
      "\n",
      "******************** topic n°3********************\n",
      "['best' 'ever' 'buy' 'far' 'yet' 'one' 'owned' 'market' 'phones' 'made']\n",
      "\n",
      "\n",
      "******************** topic n°4********************\n",
      "['screen' 'plus' 'size' 'bigger' 'smaller' 'big' 'larger' 'perfect' '6s'\n",
      " 'edge']\n",
      "\n",
      "\n",
      "******************** topic n°5********************\n",
      "['no' 'one' 'issues' 'far' 'complaints' 'problem' 'button' 'problems'\n",
      " 'home' 'got']\n",
      "\n",
      "\n",
      "******************** topic n°6********************\n",
      "['battery' 'life' 'long' 'fast' 'day' 'longer' 'lasts' 'last' 'excellent'\n",
      " 'charge']\n",
      "\n",
      "\n",
      "******************** topic n°7********************\n",
      "['problems' 'ios' 'update' 'revealed' 'loop' 'serious' 'three' 'new'\n",
      " 'geeksunion' 'ios6']\n",
      "\n",
      "\n",
      "******************** topic n°8********************\n",
      "['use' 'easy' 'pictures' 'navigate' 'fast' 'ease' 'learn' 'set' 'apps'\n",
      " 'intuitive']\n",
      "\n",
      "\n",
      "******************** topic n°9********************\n",
      "['would' 'recommend' 'definitely' 'anyone' 'highly' 'product' 'buy' 'one'\n",
      " 'everyone' 'upgrade']\n",
      "\n",
      "\n",
      "******************** topic n°10********************\n",
      "['good' 'far' 'product' 'quality' 'really' 'price' 'review' 'job' 'pretty'\n",
      " 'thanks']\n",
      "\n",
      "\n",
      "******************** topic n°11********************\n",
      "['love' 'absolutely' 'everything' 'color' 'size' 'upgraded' 'recognition'\n",
      " 'pictures' 'products' 'facial']\n",
      "\n",
      "\n",
      "******************** topic n°12********************\n",
      "['better' 'much' 'faster' 'way' '6s' 'previous' 'one' 's6' 'phones' 'old']\n",
      "\n",
      "\n",
      "******************** topic n°13********************\n",
      "['great' 'works' 'product' 'features' 'pictures' 'takes' 'fast' 'service'\n",
      " 'price' 'looks']\n",
      "\n",
      "\n",
      "******************** topic n°14********************\n",
      "['nice' 'upgrade' 'really' 'video' 'design' 'features' 'color' 'size'\n",
      " 'pictures' 'looks']\n",
      "\n",
      "\n",
      "******************** topic n°15********************\n",
      "['awesome' 'fast' 'pictures' 'quality' 'recognition' 'takes' 'display'\n",
      " 'picture' 'screen' 'product']\n",
      "\n",
      "\n",
      "******************** topic n°16********************\n",
      "['camera' 'amazing' 'quality' 'excellent' 'fast' 'pictures' 'takes'\n",
      " 'display' 'picture' 'clear']\n",
      "\n",
      "\n",
      "******************** topic n°17********************\n",
      "['like' 'really' 'looks' 'look' 'feel' 'people' 'dont' 'seems' 'android'\n",
      " 'lot']\n",
      "\n",
      "\n",
      "******************** topic n°18********************\n",
      "['not' 'even' 'know' 'could' 'think' 'worth' 'people' 'sure' 'work' 'time']\n",
      "\n",
      "\n",
      "******************** topic n°19********************\n",
      "['new' 'features' 'loves' 'happy' 'upgraded' 'upgrade' 'many' 'still' 'old'\n",
      " 'learning']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_words=[]\n",
    "for n_topic in range(np.shape(W)[1]):\n",
    "    topic_words += [getWords(nArgMax(H[n_topic],10))]\n",
    "    \n",
    "    print(\"******************** topic n°\"+str(n_topic)+\"********************\")\n",
    "    print(topic_words[n_topic])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Lesson learned. Don't rush buying things because maybe some of them might have issues. Spend wisely tho. ?\n",
      "tokens: ['lesson', 'learned', 'not', 'rush', 'buying', 'things', 'maybe', 'might', 'issues', 'spend', 'wisely', 'tho']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return [token for token in preprocessing(text).split() if token not in stopwords]\n",
    "\n",
    "#sanity check\n",
    "print(\"text: \"+scrapping_data.iloc[10000].text )\n",
    "print(\"tokens: \"+str(tokenize(scrapping_data.iloc[10000].text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens_all = [tokenize(description) for description in scrapping_data.text.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachaizadi/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Use bigram and trigram to catch combination of 2/3 words that have a specific meaning together\n",
    "bigram = Phrases(list_tokens_all, min_count=5)\n",
    "trigram = Phrases(bigram[list_tokens_all], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachaizadi/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "tokens = list(trigram[bigram[list_tokens_all]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(47074 unique tokens: ['awesome', 'love', 'pictures', 's8', 'screen']...)\n",
      "love\n",
      "pictures\n"
     ]
    }
   ],
   "source": [
    "id2word_phones = corpora.Dictionary(tokens)\n",
    "print(id2word_phones)\n",
    "print(id2word_phones.get(1))\n",
    "print(id2word_phones.get(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11144 unique tokens: ['awesome', 'love', 'pictures', 's8', 'screen']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 5 documents or more than 30% documents\n",
    "id2word_phones.filter_extremes(no_below=5, no_above=0.3)\n",
    "print(id2word_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word_phones.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 's8', 'awesome', 'screen', 'takes_great', 'pictures', 'love', 's8', 'awesome', 'screen', 'takes_great', 'pictures']\n",
      "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA (computation time should be between 5 to 60 seconds)\n",
    "\n",
    "# choose the number of topics => to find a \"good\" number of topics, try multiple values and see which one is the best\n",
    "# optionally: input alpha and eta to influence how topics are distributed across documents, \n",
    "#  and how words are distributed across topics\n",
    "#  the syntax is the following\n",
    "#  alpha is a vector of size the number of documents, and eta's size is the number of words\n",
    "\n",
    "# alpha = [0.01] * id2word_phones.num_docs\n",
    "# eta = [0.01] * len(id2word_phones.keys())\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "# Below without alpha nor eta\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=id2word_phones, passes=2)\n",
    "\n",
    "# Below with alpha and eta\n",
    "# lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=id2word_phones, passes=4, \n",
    "                            #alpha=[0.01] * id2word_phones.num_docs, eta = [0.01] * len(id2word_phones.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0.21432479), (13, 0.71644443)]\n",
      "[(3, 0.09925251), (5, 0.28295398), (6, 0.07169358), (7, 0.28365943), (14, 0.21244048)]\n"
     ]
    }
   ],
   "source": [
    "num_documents = 2 # increase this number to check how topics are distributed across more than the first 2 documents\n",
    "for i in range(num_documents):\n",
    "    print(lda_model[corpus[i]]) # what proportion of topics in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"s8\" + 0.029*\"10\" + 0.020*\"8_plus\" + 0.016*\"4\" + 0.015*\"yeah\" + 0.015*\"de\" + 0.014*\"se\" + 0.014*\"would_recommend\" + 0.013*\"no\" + 0.013*\"x\"'),\n",
       " (1,\n",
       "  '0.071*\"buy\" + 0.018*\"got\" + 0.018*\"mobile\" + 0.016*\"one\" + 0.015*\"loop_three_new_revealed\" + 0.015*\"version\" + 0.013*\"incredible\" + 0.013*\"serious_problems_ios_update\" + 0.012*\"black\" + 0.012*\"fixed\"'),\n",
       " (2,\n",
       "  '0.033*\"video\" + 0.023*\"always\" + 0.014*\"s8\" + 0.014*\"years\" + 0.013*\"cameras\" + 0.012*\"phones\" + 0.012*\"vs\" + 0.011*\"user\" + 0.011*\"future\" + 0.010*\"android\"'),\n",
       " (3,\n",
       "  '0.046*\"not\" + 0.022*\"face_id\" + 0.014*\"notch\" + 0.012*\"screen\" + 0.011*\"touch_id\" + 0.011*\"lol\" + 0.011*\"even\" + 0.010*\"would\" + 0.009*\"think\" + 0.009*\"agree\"'),\n",
       " (4,\n",
       "  '0.034*\"perfect\" + 0.027*\"s8\" + 0.027*\"phone\" + 0.018*\"please\" + 0.016*\"display\" + 0.013*\"wait\" + 0.012*\"came\" + 0.011*\"best\" + 0.011*\"not\" + 0.011*\"64gb\"'),\n",
       " (5,\n",
       "  '0.056*\"not\" + 0.024*\"people\" + 0.015*\"note_8\" + 0.015*\"buy\" + 0.015*\"would\" + 0.012*\"one\" + 0.012*\"know\" + 0.012*\"want\" + 0.011*\"think\" + 0.011*\"get\"'),\n",
       " (6,\n",
       "  '0.043*\"review\" + 0.031*\"thanks\" + 0.031*\"im\" + 0.017*\"another\" + 0.016*\"video\" + 0.016*\"waiting\" + 0.014*\"thank\" + 0.013*\"s8\" + 0.012*\"major\" + 0.011*\"good\"'),\n",
       " (7,\n",
       "  '0.053*\"screen\" + 0.045*\"not\" + 0.039*\"phone\" + 0.025*\"size\" + 0.024*\"like\" + 0.020*\"big\" + 0.011*\"home_button\" + 0.010*\"use\" + 0.010*\"smaller\" + 0.010*\"hand\"'),\n",
       " (8,\n",
       "  '0.038*\"not\" + 0.022*\"u\" + 0.017*\"phone\" + 0.016*\"update\" + 0.014*\"problem\" + 0.013*\"like\" + 0.011*\"man\" + 0.010*\"get\" + 0.010*\"no\" + 0.010*\"s7_edge\"'),\n",
       " (9,\n",
       "  '0.063*\"8\" + 0.051*\"7\" + 0.029*\"6\" + 0.024*\"not\" + 0.021*\"android\" + 0.016*\"like\" + 0.016*\"ios\" + 0.015*\"upgrade\" + 0.014*\"6s\" + 0.014*\"better\"'),\n",
       " (10,\n",
       "  '0.043*\"s8\" + 0.018*\"get\" + 0.018*\"not\" + 0.013*\"today\" + 0.010*\"come\" + 0.010*\"smartphone\" + 0.010*\"offer\" + 0.009*\"replacement\" + 0.009*\"us\" + 0.008*\"post\"'),\n",
       " (11,\n",
       "  '0.037*\"not\" + 0.027*\"phone\" + 0.018*\"would\" + 0.018*\"much\" + 0.015*\"better\" + 0.014*\"design\" + 0.012*\"like\" + 0.011*\"one\" + 0.011*\"charge\" + 0.009*\"case\"'),\n",
       " (12,\n",
       "  '0.314*\"x\" + 0.021*\"8\" + 0.017*\"new\" + 0.014*\"get\" + 0.012*\"dont\" + 0.011*\"watch\" + 0.010*\"expensive\" + 0.010*\"youtube\" + 0.009*\"cant\" + 0.009*\"3\"'),\n",
       " (13,\n",
       "  '0.144*\"phone\" + 0.083*\"great\" + 0.068*\"love\" + 0.048*\"camera\" + 0.030*\"new\" + 0.023*\"good\" + 0.023*\"amazing\" + 0.021*\"best\" + 0.021*\"awesome\" + 0.017*\"features\"'),\n",
       " (14,\n",
       "  '0.025*\"s8\" + 0.019*\"s8_plus\" + 0.014*\"bad\" + 0.012*\"seen\" + 0.012*\"not\" + 0.012*\"absolutely_love\" + 0.011*\"s7\" + 0.010*\"ones\" + 0.010*\"anyone\" + 0.010*\"hate\"'),\n",
       " (15,\n",
       "  '0.089*\"phone\" + 0.042*\"not\" + 0.014*\"get\" + 0.012*\"unlocked\" + 0.012*\"good\" + 0.012*\"store\" + 0.010*\"working\" + 0.010*\"verizon\" + 0.010*\"like\" + 0.010*\"service\"'),\n",
       " (16,\n",
       "  '0.038*\"phone\" + 0.024*\"better\" + 0.018*\"s8\" + 0.016*\"day\" + 0.012*\"best\" + 0.011*\"good\" + 0.010*\"loving\" + 0.010*\"last\" + 0.010*\"battery\" + 0.010*\"also\"'),\n",
       " (17,\n",
       "  '0.034*\"not\" + 0.023*\"no\" + 0.015*\"one\" + 0.013*\"people\" + 0.011*\"need\" + 0.010*\"like\" + 0.010*\"get\" + 0.010*\"mine\" + 0.009*\"something\" + 0.008*\"gt\"'),\n",
       " (18,\n",
       "  '0.048*\"phone\" + 0.048*\"not\" + 0.013*\"also\" + 0.012*\"use\" + 0.011*\"glass\" + 0.009*\"issues\" + 0.009*\"case\" + 0.008*\"like\" + 0.008*\"app\" + 0.007*\"cannot\"'),\n",
       " (19,\n",
       "  '0.026*\"phone\" + 0.021*\"bought\" + 0.013*\"loves\" + 0.013*\"s8\" + 0.012*\"not\" + 0.012*\"watching\" + 0.011*\"bixby\" + 0.010*\"cheaper\" + 0.010*\"replace\" + 0.009*\"plan\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
